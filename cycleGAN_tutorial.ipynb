{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN 手把手教學"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.生成器與鑑別器建置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ReflectionPad2d-1          [-1, 3, 262, 262]               0\n",
      "            Conv2d-2         [-1, 64, 256, 256]           9,472\n",
      "    InstanceNorm2d-3         [-1, 64, 256, 256]               0\n",
      "              ReLU-4         [-1, 64, 256, 256]               0\n",
      "            Conv2d-5        [-1, 128, 128, 128]          73,856\n",
      "    InstanceNorm2d-6        [-1, 128, 128, 128]               0\n",
      "              ReLU-7        [-1, 128, 128, 128]               0\n",
      "            Conv2d-8          [-1, 256, 64, 64]         295,168\n",
      "    InstanceNorm2d-9          [-1, 256, 64, 64]               0\n",
      "             ReLU-10          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-11          [-1, 256, 66, 66]               0\n",
      "           Conv2d-12          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-13          [-1, 256, 64, 64]               0\n",
      "             ReLU-14          [-1, 256, 64, 64]               0\n",
      "          Dropout-15          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-16          [-1, 256, 66, 66]               0\n",
      "           Conv2d-17          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-18          [-1, 256, 64, 64]               0\n",
      "    ResidualBlock-19          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-20          [-1, 256, 66, 66]               0\n",
      "           Conv2d-21          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-22          [-1, 256, 64, 64]               0\n",
      "             ReLU-23          [-1, 256, 64, 64]               0\n",
      "          Dropout-24          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-25          [-1, 256, 66, 66]               0\n",
      "           Conv2d-26          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-27          [-1, 256, 64, 64]               0\n",
      "    ResidualBlock-28          [-1, 256, 64, 64]               0\n",
      "  ConvTranspose2d-29        [-1, 128, 128, 128]         295,040\n",
      "   InstanceNorm2d-30        [-1, 128, 128, 128]               0\n",
      "             ReLU-31        [-1, 128, 128, 128]               0\n",
      "  ConvTranspose2d-32         [-1, 64, 256, 256]          73,792\n",
      "   InstanceNorm2d-33         [-1, 64, 256, 256]               0\n",
      "             ReLU-34         [-1, 64, 256, 256]               0\n",
      "  ReflectionPad2d-35         [-1, 64, 262, 262]               0\n",
      "           Conv2d-36          [-1, 3, 256, 256]           9,411\n",
      "             Tanh-37          [-1, 3, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 3,117,059\n",
      "Trainable params: 3,117,059\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 496.12\n",
      "Params size (MB): 11.89\n",
      "Estimated Total Size (MB): 508.76\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "\n",
    "def conv_norm_relu(in_dim, out_dim, kernel_size, stride = 1, padding=0):\n",
    "    \n",
    "    layer = nn.Sequential(nn.Conv2d(in_dim, out_dim, kernel_size, stride, padding),\n",
    "                          nn.InstanceNorm2d(out_dim), \n",
    "                          nn.ReLU(True))\n",
    "    return layer\n",
    "\n",
    "def dconv_norm_relu(in_dim, out_dim, kernel_size, stride = 1, padding=0, output_padding=0):\n",
    "    \n",
    "    layer = nn.Sequential(nn.ConvTranspose2d(in_dim, out_dim, kernel_size, stride, padding, output_padding),\n",
    "                          nn.InstanceNorm2d(out_dim), \n",
    "                          nn.ReLU(True))\n",
    "    return layer\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim, use_dropout):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        res_block = [nn.ReflectionPad2d(1),\n",
    "                     conv_norm_relu(dim, dim, kernel_size=3)]\n",
    "        \n",
    "        if use_dropout:\n",
    "            res_block += [nn.Dropout(0.5)]\n",
    "        res_block += [nn.ReflectionPad2d(1),\n",
    "                      nn.Conv2d(dim, dim, kernel_size=3, padding=0),\n",
    "                      nn.InstanceNorm2d(dim)]\n",
    "\n",
    "        self.res_block = nn.Sequential(*res_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.res_block(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_nc=3, output_nc=3, filters=64, use_dropout=True, n_blocks=2):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # 向下採樣 ( shape + 2 * padding - kernel + 1 ) / stride \n",
    "        # 256 + 3*2 = 262\n",
    "        # 262 - 7 + 0 + 1 = 256 \n",
    "        # ( 256 + 2 - 3 + 1 / 2 = 128\n",
    "        # 128 + 2 - 3 + 1 / 2  = 64\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 conv_norm_relu(input_nc   , filters * 1, 7),\n",
    "                 conv_norm_relu(filters * 1, filters * 2, 3, 2, 1),\n",
    "                 conv_norm_relu(filters * 2, filters * 4, 3, 2, 1)]\n",
    "\n",
    "        # 頸脖層\n",
    "        for i in range(n_blocks):\n",
    "            model += [ResidualBlock(filters * 4, use_dropout)]\n",
    "\n",
    "        # 向上採樣 (input-1)*stride + kernel - 2*padding +  output_padding\n",
    "        # (64-1)*2 + 3 -2 +1 = 128\n",
    "        # (128-1)*2 + 3 -2 + 1 = 256\n",
    "        # 256 + 6 = 262\n",
    "        # 262 - 7 + 1 = 256\n",
    "        model += [dconv_norm_relu(filters * 4, filters * 2, 3, 2, 1, 1),\n",
    "                  dconv_norm_relu(filters * 2, filters * 1, 3, 2, 1, 1),\n",
    "                  nn.ReflectionPad2d(3),\n",
    "                  nn.Conv2d(filters, output_nc, 7),\n",
    "                  nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)    # model 是 list 但是 sequential 需要將其透過 , 分割出來\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "G = Generator()\n",
    "summary(G, (3,256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *list 分割範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "1 2 3 4 5 6 7 8\n"
     ]
    }
   ],
   "source": [
    "list1 = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "print(list1)\n",
    "print(*list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 鑑別器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]             256\n",
      "         LeakyReLU-2         [-1, 64, 256, 256]               0\n",
      "            Conv2d-3        [-1, 128, 128, 128]         131,200\n",
      "    InstanceNorm2d-4        [-1, 128, 128, 128]               0\n",
      "         LeakyReLU-5        [-1, 128, 128, 128]               0\n",
      "            Conv2d-6          [-1, 256, 64, 64]         524,544\n",
      "    InstanceNorm2d-7          [-1, 256, 64, 64]               0\n",
      "         LeakyReLU-8          [-1, 256, 64, 64]               0\n",
      "            Conv2d-9          [-1, 512, 63, 63]       2,097,664\n",
      "   InstanceNorm2d-10          [-1, 512, 63, 63]               0\n",
      "        LeakyReLU-11          [-1, 512, 63, 63]               0\n",
      "           Conv2d-12            [-1, 1, 62, 62]           8,193\n",
      "================================================================\n",
      "Total params: 2,761,857\n",
      "Trainable params: 2,761,857\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 182.54\n",
      "Params size (MB): 10.54\n",
      "Estimated Total Size (MB): 193.83\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "\n",
    "def conv_norm_leakyrelu(in_dim, out_dim, kernel_size, stride = 1, padding=0, output_padding=0):\n",
    "\n",
    "    layer = nn.Sequential(nn.Conv2d(in_dim, out_dim, kernel_size, stride, padding),\n",
    "                          nn.InstanceNorm2d(out_dim), \n",
    "                          nn.LeakyReLU(0.2,True))\n",
    "    return layer\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_nc=3, filters=64, n_layer = 3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        \n",
    "        # 第一層不做 batchNorm\n",
    "        # 256 -1 +1 = 256\n",
    "        model = [\n",
    "            nn.Conv2d(input_nc, filters, kernel_size=1, stride=1, padding=0),\n",
    "            nn.LeakyReLU(0.2, True)]\n",
    "        \n",
    "        # 第二、三層相同\n",
    "        # 256 +2 -4 +1 / 2 = \n",
    "        for i in range(1, n_layer):\n",
    "            n_filters_prev = 2**(i-1)\n",
    "            n_filters = 2**i\n",
    "            model += [conv_norm_leakyrelu(filters * n_filters_prev , filters * n_filters, kernel_size=4,\n",
    "                                           stride=2, padding=1)]\n",
    "        # 第四層 stride 為 1\n",
    "        n_filters_prev = 2**(n_layer-1)\n",
    "        n_filters = 2**n_layer\n",
    "        model += [conv_norm_leakyrelu(filters * n_filters_prev , filters * n_filters, kernel_size=4,\n",
    "                                           stride=1, padding=1)]\n",
    "        # 輸出層\n",
    "        model += [nn.Conv2d(filters * n_filters, 1, kernel_size=4, stride=1, padding=1)]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "    \n",
    "D = Discriminator()\n",
    "summary(D, (3,256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initial CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os, shutil\n",
    "from torch.nn import init\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import itertools\n",
    "\n",
    "\n",
    "###### initial ######\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "###### basic parameters ######\n",
    "        \n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 12\n",
    "epochs = 1\n",
    "decay_epoch = 10\n",
    "lr = 2e-3\n",
    "log_freq = 100\n",
    "\n",
    "############ Create Relative Dir ############\n",
    "\n",
    "weights_path = r'.\\weights'\n",
    "output_path = r'.\\results'\n",
    "\n",
    "if os.path.exists(output_path) == False:\n",
    "    os.makedirs(output_path)\n",
    "    print('Create dir : ', output_path)\n",
    "    \n",
    "if os.path.exists(weights_path) == False:\n",
    "    os.makedirs(weights_path)\n",
    "    print('Create dir : ', weights_path)\n",
    "\n",
    "############ Define Model ############\n",
    "\n",
    "G_A2B = Generator().to(device)\n",
    "G_B2A = Generator().to(device)\n",
    "D_A = Discriminator().to(device)\n",
    "D_B = Discriminator().to(device)\n",
    "\n",
    "G_A2B.apply(weights_init_normal)\n",
    "G_B2A.apply(weights_init_normal)\n",
    "D_A.apply(weights_init_normal)\n",
    "D_B.apply(weights_init_normal)\n",
    "\n",
    "############ define Loss function ############\n",
    "\n",
    "MSE = nn.MSELoss()\n",
    "L1 = nn.L1Loss()\n",
    "\n",
    "############ define optimizer ############\n",
    "\n",
    "class LambdaLR():\n",
    "    def __init__(self, epochs, offset, decay_epoch):\n",
    "        self.epochs = epochs\n",
    "        self.offset = offset\n",
    "        self.decay_epoch = decay_epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_epoch)/(self.epochs - self.decay_epoch)\n",
    "\n",
    "\n",
    "optim_G = torch.optim.Adam(itertools.chain(G_A2B.parameters(), G_B2A.parameters()), lr=lr, betas=(0.5, 0.999))\n",
    "optim_D = torch.optim.Adam(itertools.chain(D_A.parameters()  , D_B.parameters()), lr=lr, betas=(0.5, 0.999))\n",
    "        \n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optim_G,lr_lambda=LambdaLR(epochs, 0, decay_epoch).step)\n",
    "lr_scheduler_D = torch.optim.lr_scheduler.LambdaLR(optim_D, lr_lambda=LambdaLR(epochs, 0, decay_epoch).step)\n",
    "\n",
    "############ Prepare Data ############\n",
    "\n",
    "transform = transforms.Compose(\n",
    "            [transforms.RandomHorizontalFlip(),\n",
    "             transforms.Resize((256, 256)),\n",
    "             transforms.RandomCrop((224, 224)),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "train_path = r'.\\vangogh2photo'\n",
    "\n",
    "trainA_path =  os.path.join(train_path, r'trainA')\n",
    "targetA_path = os.path.join(train_path, r'new_trainA')\n",
    "\n",
    "trainB_path =  os.path.join(train_path, r'trainB')\n",
    "targetB_path = os.path.join(train_path, r'new_trainB')\n",
    "\n",
    "if os.path.exists(targetA_path) == False:\n",
    "    os.makedirs(targetA_path)\n",
    "    print('Create dir : ', targetA_path)\n",
    "    shutil.move(trainA_path, targetA_path)\n",
    "if os.path.exists(targetB_path) == False:\n",
    "    os.makedirs(targetB_path)\n",
    "    print('Create dir : ', targetB_path)\n",
    "    shutil.move(trainB_path, targetB_path)\n",
    "\n",
    "dataA_loader = DataLoader(dsets.ImageFolder(targetA_path, transform=transform), batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataB_loader = DataLoader(dsets.ImageFolder(targetB_path, transform=transform), batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 準備圖片供預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store 50 generated image in a pool and sample from it when it is full\n",
    "# Shrivastava et al’s strategy\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        assert (max_size > 0), \"Empty buffer or trying to create a black hole. Be careful.\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return torch.cat(to_return)\n",
    "    \n",
    "fake_A_sample = ReplayBuffer()\n",
    "fake_B_sample = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([12, 3, 224, 224])\n",
      "torch.Size([12])\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def show_AB():\n",
    "    img1 = data[0][0][0].numpy().transpose((1,2,0))    # vangogh\n",
    "    img2 = data[1][0][0].numpy().transpose((1,2,0))    # real pic\n",
    "    res = cv2.hconcat([img1, img2])\n",
    "\n",
    "    cv2.imshow('test' , res)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "for idx, data in enumerate(zip(dataA_loader, dataB_loader)):\n",
    "    if idx > 0 :\n",
    "        break \n",
    "    else :\n",
    "        print(len(data))           # two data loader\n",
    "        print(data[0][0].shape)    # trainA input_x\n",
    "        print(data[0][1].shape)    # trainA input_y\n",
    "        print(data[0][0].shape[0]) # get batch size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/654 [01:48<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 7552892928 bytes. Buy new RAM!\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-8f71375485be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Train G - Adversial Loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mfake_A\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG_B2A\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_B\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mfake_out_A\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mD_A\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_A\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-fa9818c3f67d>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 342\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 7552892928 bytes. Buy new RAM!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(zip(dataA_loader, dataB_loader)),\n",
    "                        total = len(data_loader))\n",
    "    \n",
    "    for idx, data in progress_bar:\n",
    "        \n",
    "        ############ define training data & label ############\n",
    "        \n",
    "        real_A = data[0][0].to(device)    # vangogh image\n",
    "        real_B = data[1][0].to(device)    # real picture\n",
    "        real_batch_size = real_A[0][0].shape[0]\n",
    "        \n",
    "        real_label = torch.ones( (real_batch_size, 1) , dtype=torch.float32).to(device)\n",
    "        fake_label = torch.zeros( (real_batch_size, 1) , dtype=torch.float32).to(device)\n",
    "        \n",
    "        \n",
    "        ############ Train G ############\n",
    "        \n",
    "        optim_G.zero_grad()\n",
    "        \n",
    "        # Train G - Adversial Loss  \n",
    "        \n",
    "        fake_A = G_B2A(real_B)\n",
    "        fake_out_A = D_A(fake_A)\n",
    "        \n",
    "        fake_B = G_A2B(real_A)\n",
    "        fake_out_B = D_B(fake_B)\n",
    "        \n",
    "        adversial_loss_B2A = MSE(fake_out_A, real_label)\n",
    "        adversial_loss_A2B = MSE(fake_out_B, real_label)\n",
    "        \n",
    "        adv_loss = adversial_loss_B2A + adversial_loss_A2B\n",
    "        \n",
    "        # G - Consistency Loss (Reconstruction)  \n",
    "        \n",
    "        rec_A = G_B2A(fake_B)\n",
    "        rec_B = G_A2B(fake_A)\n",
    "        \n",
    "        consistency_loss_B2A = L1(rec_A, real_A)\n",
    "        consistency_loss_A2B = L1(rec_B, real_B)\n",
    "        \n",
    "        rec_loss = consistency_loss_B2A + consistency_loss_A2B\n",
    "        \n",
    "        # G - Identity  Loss  \n",
    "        \n",
    "        idt_A = G_B2A(real_A)\n",
    "        idt_B = G_A2B(real_B)\n",
    "        \n",
    "        identity_loss_A = L1(idt_A, real_A)\n",
    "        identity_loss_B = L1(idt_B, real_B)\n",
    "        \n",
    "        idt_loss = identity_loss_A + identity_loss_B\n",
    "        \n",
    "        # G - Total Loss  \n",
    "        \n",
    "        beta_rec = 10\n",
    "        beta_idt = 5\n",
    "        \n",
    "        loss_G = adv_loss + ( rec_loss * beta_rec ) + ( idt_loss * beta_idt )\n",
    "        \n",
    "        # G - Backward & Update  \n",
    "        \n",
    "        loss_G.backward()\n",
    "        optim_G.step()\n",
    "        \n",
    "        ############ Train D ############\n",
    "        \n",
    "        optim_D.zero_grad()\n",
    "        \n",
    "        # D - Adversial D_A Loss  \n",
    "        \n",
    "        real_out_A = D_A(real_A)\n",
    "        real_out_A_loss = MSE(real_out_A, real_label)\n",
    "        \n",
    "        fake_out_A = D_A(fake_A_sample.push_and_pop(fake_A))\n",
    "        fake_out_A_loss = MSE(real_out_A, fake_label)\n",
    "        \n",
    "        loss_DA = real_out_A_loss + fake_out_A_loss\n",
    "        \n",
    "        # D - Adversial D_B Loss  \n",
    "        \n",
    "        real_out_B = D_B(real_B)\n",
    "        real_out_B_loss = MSE(real_out_B, real_label)\n",
    "        \n",
    "        fake_out_B = D_B(fake_B_sample.push_and_pop(fake_B))\n",
    "        fake_out_B_loss = MSE(fake_out_B, fake_label)\n",
    "        \n",
    "        loss_DB = ( real_out_B_loss + fake_out_B_loss )\n",
    "        \n",
    "        # D - Total Loss \n",
    "        \n",
    "        loss_D = ( loss_DA + loss_DB ) * 0.5\n",
    "        \n",
    "        # Backward & Update\n",
    "        \n",
    "        loss_D.backward()\n",
    "        optim_D.step()\n",
    "        \n",
    "        ############ progress info ############\n",
    "        \n",
    "        progress_bar.set_description(\n",
    "            f\"[{epoch}/{epochs - 1}][{idx}/{len(dataloader) - 1}] \"\n",
    "            f\"Loss_D: {(loss_DA + loss_DB).item():.4f} \"\n",
    "            f\"Loss_G: {loss_G.item():.4f} \"\n",
    "            f\"Loss_G_identity: {(idt_loss).item():.4f} \"\n",
    "            f\"loss_G_GAN: {(adv_loss).item():.4f} \"\n",
    "            f\"loss_G_cycle: {(rec_loss).item():.4f}\")\n",
    "        \n",
    "        if i % log_freq == 0:\n",
    "            \n",
    "            vutils.save_image(real_A, f\"{output_path}/real_A_{epoch}.jpg\", normalize=True)\n",
    "            vutils.save_image(real_B, f\"{output_path}/real_B_{epoch}.jpg\", normalize=True)\n",
    "            \n",
    "            fake_A = ( G_B2A( real_B ).data + 1.0 ) * 0.5\n",
    "            fake_B = ( G_A2B( real_A ).data + 1.0 ) * 0.5\n",
    "            \n",
    "            vutils.save_image(fake_A, f\"{output_path}/fake_A_{epoch}.jpg\", normalize=True)\n",
    "            vutils.save_image(fake_B, f\"{output_path}/fake_A_{epoch}.jpg\", normalize=True)\n",
    "        \n",
    "        \n",
    "    torch.save(netG_A2B.state_dict(), f\"weights/netG_A2B_epoch_{epoch}.pth\")\n",
    "    torch.save(netG_B2A.state_dict(), f\"weights/netG_B2A_epoch_{epoch}.pth\")\n",
    "    torch.save(netD_A.state_dict(), f\"weights/netD_A_epoch_{epoch}.pth\")\n",
    "    torch.save(netD_B.state_dict(), f\"weights/netD_B_epoch_{epoch}.pth\")\n",
    "\n",
    "    ############ Update learning rates ############\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D.step()\n",
    "\n",
    "############ save last check pointing ############\n",
    "torch.save(netG_A2B.state_dict(), f\"weights/netG_A2B.pth\")\n",
    "torch.save(netG_B2A.state_dict(), f\"weights/netG_B2A.pth\")\n",
    "torch.save(netD_A.state_dict(), f\"weights/netD_A.pth\")\n",
    "torch.save(netD_B.state_dict(), f\"weights/netD_B.pth\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
